---
title: "Delete-d Jackknife Validation"
output: html_notebook
---

## Delete-d Jackknife Validation

Sensitive metrics are assumed to reflect ecological response to an environmental gradient. However, each multi-metric index is susceptible to overfitting of the data (Barbour et al. 1996). In this case, overfitting refers to the selection of metric(s) that appear to reflect an ecological response to the disturbance gradient but in fact reflect random variability or nuances of the data set used to construct the index. Validation procedures verify that the index measures an ecological response to a defined gradient, and thus, protects against overfitting.

In general, validation requires the data set to be divided into a training set and a validation set prior to index development (Southerland et al. 2005, Pond et al. 2011). The training set is used to develop the index, while the validation set is used to verify that the index classifies data appropriately. When sample size is small, it may not be possible to set aside an independent dataset for validation purposes (Hawkins 2004). In such instances, Cross Validation (CV) can be used to create and validate an index with the same dataset. The Delete-d Jackknife CV procedure was used to validate each bioregion index. Buchanan et al. (2011) referred to this method as a jackknife with replacement but this is more frequently referred to as a Delete-d Jackknife. This is an iterative process creating a unique training dataset and validation dataset with each iteration. For each iteration, d samples are removed from the dataset to form a validation dataset; the remaining samples constitute the training dataset. A true Delete-d Jackknife removes d samples and re-computes the final value (e.g., mean, median, or CE) for each possible data combination. This quickly becomes computationally impossible for the average desktop computer. For example, with a sample size of 100 and d equal to 25 there are greater than 2.4 x 1023 possible combinations. Therefore, five-hundred unique Delete-d Jackknife combinations were used in this study as an estimate of the results of all the possible combinations.

A portion of the samples (d) in the Reference and Degraded populations were randomly removed. Shao and Wu (1989) recommend that d should be greater than the square root of n but less than n (‚àöùëõ < d < n). Buchanan et al. (2011) removed 10% of the reference population during CV but 10% of any bioregion with one-hundred or fewer reference samples would produce a d value lower than the recommended range. Therefore, we set d equal to 25% of the Reference population. Additionally, 25% of the Degraded population was also removed during the CV procedure because the Degraded distribution in combination with the Reference distribution influences metric scoring thresholds. The removal of 25% of Reference and 25% of Degraded samples placed d well within the range recommended by Shao and Wu (1989) for all indices.

All the available and applicable data within each spatial resolution was used to develop the appropriate Chesapeake-wide, region, or bioregion specific indices. A Delete-d Jackknife CV was used to verify that the index was not overfit to the data. Five-hundred CV iterations were conducted. With each iteration of the CV process the index was reconstructed with a unique training set and CE was checked using the corresponding, independent validation set. The CV tests utilized only the metrics selected when using all the data to build the index. The goal of this process was to test the validity of the original index. Therefore, allowing the program to deviate from the metrics originally selected using all the data would not address the accuracy of the original index. CE of the validation set calculated with each iteration was used to calculate the expected CE and RMSE. Mean simulated CE was the average CE of all iterations (ùúÉÃÇ(.)). RMSE provides a measure of standard deviation associated with the expected CE (Equation 7).

_Equation 7._

    Where:

The CV method described above is an iterative modification of the validation process typically found in the literature. Instead of parsing the data into training and validation sets prior to development, the index was developed using all the data and post-development the data was iteratively parsed into training and validation data sets. Although both methodologies verify that the index reflects ecological responses to an environmental gradient, the CV method should provide a more robust assessment because the validation process was repeated five-hundred times.