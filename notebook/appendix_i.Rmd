---
title: "Appendix I: Index Methodologies"
output: html_notebook
---

## Appendix I: Index Methodologies

Karrâ€™s (1981) original index of biotic integrity (IBI) included twelve equally weighted metrics selected to represent different aspects of the fish community. The twelve metrics were individually scored and the scores averaged to produce the index score of a sampling event. Most IBIâ€™s do not deviate drastically from this formula. We tested eight methods of constructing the index in each of the twelve identified bioregions (Figure I-1). We used custom R-scripts to run the tests on family-level metrics. Several of the methods were not intended to be used in the development of the final Chessie BIBI indices but were instead done to gain insight into the effects of different index structures and metrics. We did not remove repeat samples collected at the same station, but this should not affect comparisons of the eight methods.

### Methods

The distributions of biological metric values in Reference and Degraded conditions are used to establish metric scoring protocols. Therefore, the choice of criteria for defining Reference and Degraded is critical. In the following analysis, the Reference criteria are identical to those used in the bioregion-specific Chessie BIBI refinement (Tables 2, 3). The Degraded criteria, however, are somewhat relaxed:

    Stricter Degraded criteria used in Chessie BIBI refinement:
        > 50% of available habitat scores < 6
        and
        at least two of the three water quality parameters exceeding minimally stressful thresholds or one parameter exceeding moderately stressful thresholds
    
    Relaxed Degraded criteria used in this analysis:
        > 50% of available habitat scores < 6
        or
        at least one of the three water quality parameters exceeding minimally stressful thresholds

This was to allow sufficient numbers of Degraded samples for jackknife validation steps. Basic characteristics of the twelve bioregions and their numbers of Reference and Degraded samples are shown in Table I-1.

All metrics were scored according to procedures described in Report Section II.G. Metric Scoring Approach. The Best Separation Point (BSP) and the median of a metricâ€™s Reference distribution are used to establish the â€œmiddleâ€ and â€œceilingâ€ of the scoring scale of 0 â€“ 100. The Best Separation Point (BSP) is the metric value that yields the highest Balanced Discrimination Efficiency (BDE) between Reference and Degraded distributions of the metric. The â€œfloorâ€ of the scoring scale is established by extrapolating linearly from the Reference median and the BSP. Metric values between the floor and ceiling thresholds are scored on a continuous gradient from 0 â€“ 100. Metric values greater than the ceiling threshold are scored 100; values less than the floor are scored 0.

_Figure I-1. Metric composition of the eight possible multi-metric indices tested. â€œXâ€ indicates the metric was included in that methodâ€™s index. Metrics selected by the computer program were assessed with range, variability, and redundancy tests. For methods D â€“ H, the metric scores are first averaged by metric category. Then, metric category averages are averaged to obtain an index score. See Table I-1 and text for details._

_Table I-1. Bioregion characteristics and numbers of Reference and Degraded samples. Mean slope is the area-weighted average of all HUC12 watershed mean slopes in a bioregion. Mean elevation is the area-weighted average of all HUC12 watershed mean elevations in a bioregion. REF, Reference; DEG, Degraded._

### Index Development Methodologies

In general, IBI's are developed by assessing sensitivity, range, variability, and redundancy of multiple metrics and then selecting representative metrics to include in the final index (Report Section II.F. Metric Testing). Method A was developed with this general approach. It should be considered a baseline from which to compare the remaining methodologies. For Method A, the metrics are divided into five metric categories (i.e., Richness/Diversity, Tolerance, Functional Feeding Groups, Habit, and Composition) and the most sensitive metric from each category is selected to represent the category. Of the five categories, the four most sensitive metrics are first selected to be the foundation of the index. This means that one category is not represented initially. Additional sensitive metrics are then added to the index only if they improve the Classification Efficiency (CE). Metrics omitted during the selection of the top four metrics are readmitted if the metric improves the final CE.

Method B represents one extreme in index development. The index includes all 84 of the available metrics in the five metric categories (Table E-1), the % Family metrics for all families present in the sample, and the percentage metrics for Phylum, Subphylum, Class, Subclass, Order, and Suborder. The index score is the mean of all of the metric scores. No range, variability, and redundancy checks are performed and no zero inflation protection is applied. Zero inflation is a common issue with ecological data and occurs when abundance values for a given taxon are dominated by zeros and mask underlying differences between Reference and Degraded conditions (McCune et al. 2002, Martin et al. 2005). Thresholds selected by the program for scoring metrics can be forced to zero if a taxon is rare and its abundance distribution includes many zeros.

Method C is similar to method B but protects against zero inflation. To alleviate the dominance of zeros, the percentage of Reference samples containing zero values are assessed and the zeros are omitted from the metric sensitivity analysis if more than 5% of the samples are represented by zero. BSP and Reference median values are set according to distributions of non-zero values in Reference and Degraded conditions. Instances where metric zero values were removed are not scored, and a sampleâ€™s index score is calculated from the average of the remaining metric scores.

The index scores in Methods A â€“ C above are calculated by averaging the individual metric scores, regardless of category. In Methods D â€“ H below, metric scores are first grouped and averaged by metric category. Then, the five (or six) category scores are averaged to calculate the index score. It is important to represent a diversity of metric types in an index to ensure that the index captures the biological communityâ€™s responses to a range of possible stressors (Karr 1981, Karr et al. 1986, Barbour et al. 1999). Preliminary analyses of the Chesapeake Bay basin macroinvertebrate data showed that representing most or all of the metric categories in the index allows for the inclusion of metrics that are often excluded due to low sensitivity, low range, or high variability. Although these individual metrics do not perform as strongly, they can improve an indexâ€™s overall sensitivity. Initial analyses also suggested that averaging the scores of multiple metrics in each category improved the stability of the final index. Random variability associated with a single metric can incorrectly indicate Reference or Degraded conditions but the average of multiple metrics in a metric category counter-balanced that error.

Method D is similar to Method A but metrics are grouped and averaged by metric category before the index is calculated and zero inflation protection is applied. The program checks the range, variability, and redundancy of the possible metrics in each metric category and then selects metrics to include in the category. No limits are set on the number of metrics that can be included in a category. A category can be excluded from the index if metrics within the category do not meet the selection criteria or are insensitive to disturbance.

Method E follows the same procedures as Method D for the Richness/Diversity and Composition categories but pre-selects the FFG, Habit, and Tolerance category metrics to be included in each bioregion-specific index (Figure I-1). FFG, Habit, and Tolerance metrics are often excluded from other indices because they involve rare taxa or groups. For example, the % Predator metric is typically excluded from indices because the percentage of predators usually is not greater than ~10% (Vannote 1981). Preliminary analyses showed that when zero values are removed and only non-zero values are evaluated, % Predator showed substantial declines with disturbance. The benefit of pre-selecting metrics for the FFG, Habit, and Tolerance categories is that, rather than one or no metrics representing a particular category, an average of multiple metric scores represent the category and can provide a more holistic assessment of the category.

For Methods F, G, and H, all of the metrics are pre-selected and there is no assessment of metric range, variability, or redundancy. Zero inflation protection is applied. Method F, G, and H test 4 â€“ 6 pre-selected metrics in each category for a total of 24 metrics (Figure I-1). Methods G and H also consider the suite of percent family metrics (% Chironomidae, % Hydrophilidae, % Tipulidae, etc.). An R script was developed to find the percentages of all families identified in a sample. Method G combines the Composition metrics listed in Figure I-1 and the scores of the percent family metrics. Method H creates a sixth metric category that contains the scores of all the percent family metrics.

A delete-d Jackknife procedure was used to assess the precision of the CEs obtained with each method (See Report Section II.I. Index Classification Efficiency). In each of 100 iterations, 25% of the Reference and Degraded data are removed and figuratively discarded. The index is reconstructed on the remaining 75% Reference and Degraded data using the same biological metrics chosen in the original index development. The metrics are rescored with the resulting new thresholds, and the index scores are used to calculate a CE for the smaller dataset. Differences between the CEs of the 100 iterations and the mean of those CEs are then used to calculate Root Mean Standard Error (RMSE). Variation around the mean CE indicates how much variability can be expected based on the â€œtrainingâ€ dataset used to construct the index.

In addition, a delete-d Jackknife Cross Validation (CV) procedure was used to assess the accuracy of the CEs obtained with each method (see Report Section II.I. Index Classification Efficiency). In each of 100 iterations, the bioregion-specific Reference and Degraded data are divided into training (75%) and validation (25%) sets. The index is reconstructed on the training set using the same biological metrics chosen in the original index development. The metrics are rescored with the resulting new thresholds, and the index scores are used to calculate a CE for the training dataset. The new metric thresholds are then used to score the validation dataset and the index scores are used to calculate that datasetâ€™s CE. Differences between the CEs of the 100 iterations of the validation dataset and the mean of those CEs are then used to calculate validation datasetâ€™s RMSE. Variation around the mean indicates how well the index would perform on an independent dataset.

### Results

All eight methods produce index scores that clearly separate the macroinvertebrate assemblages from Reference and Degraded conditions (Table I-2). The overall mean CE derived from the complete dataset varies only 3.2% across the eight methods, from 74.4% (Method F) to 77.6% (Method C). The index is constructed to identify Reference and Degraded equally well, which amounts to about three-quarters of Reference samples and three-quarters of Degraded samples correctly identified. The delete-d Jackknife Cross Validation (CV) test for accuracy and the delete-d Jackknife test for precision produce CEs results very similar to the original CEs. For the former, overall mean CE varies 3.1% between methods, from 72.6% (Method D) to 75.8% (Method A). For the latter, overall mean CE various 2.9% between methods, from 75.2% (Method D) to 78.1 (Method C).

_Table I-2. Classification Efficiencies (CE) for eight index methodologies (A â€“ H) in twelve Chesapeake bioregions. Original, CE of the full dataset. CV mean, the average CE produced by a delete-d Jackknife Cross Validation (CV) procedure. Jackknife mean, the average CE produced by a delete-d Jackknife Precision procedure. The means of all bioregion CE values indicates the overall performance of each method. See Table I-1 and text for names and descriptions of bioregions. See text for details of the jackknife procedures._

The original, jackknife CV, and jackknife precision CEs for each individual bioregions are also very close. Differences between bioregions, however, can be large. The NAPU bioregion, followed by NRV and SRV have the lowest CEs of each method tested. These are the three largest bioregions (Table I-1). UNP and LNP have the highest CEs of each method tested. These bioregions are moderately sized with relatively large numbers of Reference and Degraded samples. The remaining seven bioregions have intermediate CEs, are small to moderately sized, and some have relatively few numbers of Reference and/or Degraded samples. Variation in the bioregion CEs does not correspond to obvious environmental differences such as area-weighted mean slope, range of watershed mean slopes, or area-weighted mean elevation (Table I-1).

The RMSE estimates generated from the delete-d Jackknife Cross Validation (CV) test for accuracy in each bioregion are only a few percent larger than the corresponding RMSE generated from the delete-d Jackknife test for precision (Figure I-2). Also, the CEs developed from the original, complete dataset typically fall inside the RMSE ranges. The metric scoring thresholds developed on the training datasets are accurately classifying the validation datasets. Changes in the reconstructed metric scoring thresholds over the 100 jackknife iterations are minimal and introduce little new uncertainty in the index values and CEs. Most of the uncertainty appears to be driven by inherent variability in the macroinvertebrate samples used to create the indices. It is worth noting that bioregions with the largest RSME either had very few Reference samples (MAC), very few Degraded samples (BLUE), or very few of both (CA).

_Figure I-2. Classification Efficiencies (CE) for eight methods of index development in twelve bioregions (see Table I-1 for bioregion names). Left, the blue circle represents the mean CE value of the validation datasets generated in 100 jackknife iterations. Right, the blue circle represents the mean CE value of the training datasets generated in 100 jackknife iterations. Extending from the blue circles are the RSME values calculated from the 100 iterations. Red triangle, the original CE of the index developed with all the data (for comparison purposes)._

Despite application of the same water quality and stream habitat criteria to identify Reference and Degraded conditions, distributions of index scores in Reference and Degraded are not identical across the eight methods (Figure I-3). Reference distributions in UNP and LNP are typically more compact and have the highest medians across all methods while MAC and SRV have the lowest median values. Reference distributions in NAPU, NRV, and SEP have the largest interquartile ranges. Degraded distributions of the index scores also vary. UNP and LNP have the lowest median scores in each of the eight methods; MAC, CA, NAPU, SRV, and PIED had the higher median scores. UNP and LNP have the highest CEs as a result of the large separation between their Reference and Degraded distributions. The persistent overlap in the Reference and Degraded distributions of NAPU, NRV and SRV in all eight methods exemplifies their low CEs.

_Figure I-3 (next four pages). Bioregion-specific, Reference (REF, blue) and Degraded (DEG, white) distributions of index scores produced with the eight index constructs. Box-and-whisker plots indicate 95th, 75th, median, 25th, and 5th percentiles; â™¦, 10th percentile of Reference distribution. Median of the Reference medians and Degraded medians shown on the right. See Figure I-1 for method descriptions and Table 4 for bioregion names._

### Discussion

Within each bioregion, the eight methods of constructing an index are equally sensitive and none of the methods outperforms the others in terms of CE. The CEs and their associated variability (RSME) are effectively the same regardless of whether the index for a given bioregion is constructed with the most sensitive metrics, averages of metric categories, the same suite of metrics, or all available metrics. We believe this is due in large part to a counter-balancing effect in the metric scoring. Metrics in a Reference-quality sample that score low are typically countered by a larger number of metrics that score high, resulting in an overall high index score that classifies the sample correctly as Reference-like. Similarly, a metric in a Degraded-quality sample that scores high is typically countered by several that score low, correctly classifying the sample as Degraded-like. The key is to have sufficient metrics so the counter-balancing effect can occur. The Method A indices are the average of 5 to 16 individual metrics, depending on bioregion; Method D indices are the average of 5 metric category averages derived from 5 or more program-picked metrics; Method F indices are the average of 5 metric category averages derived from 24 standard metrics; Method C indices have upwards of 150 metrics; and Method B indices have 272 metrics. Based on our results it would appear that five metrics are sufficient to achieve this counter-balancing effect.

Within each bioregion, variation in the Reference and Degraded distributions across the eight methods is mostly related to the methodsâ€™ component metrics and scoring approaches. Method B scores are universally high and have the smallest separation between Reference and Degraded because the method lacks zero inflation protection. Method C is identical to B but has zero inflation protection. It pulls Reference and Degraded scores towards the middle of the 0 â€“ 100 index scale because numerous mediocre and weak metrics are included. Methods F, G, and H are fairly well separated and produce nearly identical results. They are composed of a standard suite of mostly sensitive metrics and employ zero inflation protection. The addition of a sixth group, Families, in Method H does not significantly change the Reference and Degraded index distributions. The appeal of these three methods is they use a consistent suite of richness/diversity, tolerance, FFG, and habit metrics across all bioregions. Finally, Methods D and A show the largest separation between Reference and Degraded distributions. They allow the R-program to select and include the most discriminating metrics in all five metric categories, and they employ zero inflation protection. One drawback is they are made up of different metrics. Sensitive metrics like EPT might not be selected for inclusion in a given bioregion index because the group is less abundant in that bioregion. For example, EPT represents a smaller percentage of the assemblages in MAC and SEP compared to the other bioregions (Figure G-8g). EPT-based metrics are not selected by Method A for the MAC and SEP bioregions but are selected frequently for other bioregions.

Within each index method, a consistent pattern of differences occurs across the twelve bioregions in the original CEs and in the overall distributions of Reference and Degraded index scores. Variation in the quality of Reference and Degraded conditions is perhaps the most obvious factor causing these bioregion differences. For example, the BLUE bioregion can be considered a minimally disturbed bioregion with high quality Reference conditions because it is mostly forested and largely within the protected Shenandoah National Forest. Of the 446 stream samples from BLUE, 134 (30.0%) were identified as coming from sites that meet the Reference water quality and stream habitat criteria. All the Reference sites were in predominantly forested watersheds. In contrast, the coastal MAC region is heavily impacted by human activity and true Reference sites are difficult to find. Most of MAC watersheds have less than 30.0% forest cover, most of the bioregionsâ€™ stream channels have been altered by ditching, and agriculture draws heavily on groundwater aquifers and affects baseflow. Of the 1,589 stream samples collected in MAC, only 21 meet the standardized habitat and water quality criteria for Reference, the lowest number for any bioregion. None are in predominantly forested watersheds. The consistent differences between BLUE and MAC distributions of Reference index scores across all eight index methods can be largely attributed to the fact that Reference conditions in BLUE are of a higher quality than in MAC, even though both Reference groups meet the studyâ€™s standard water quality and habitat criteria for Reference.

Bioregional differences in the Reference index score distributions may also be related to natural features which can alter macroinvertebrate assemblages and taxa behaviors. The influences of natural factors are confounded to some extent by differences in the quality of Reference conditions (mentioned above), but can be discerned in some cases. Take the CA, NCA, BLUE, and SRV bioregions that are heavily forested but have different mean slopes (Table I-1). Low gradient, slow-moving, soft-bottomed streams present challenges and opportunities for taxa that differ from higher gradient, faster-moving, hard-bottomed streams. The median percentage of burrowers in Reference conditions is 24.0% in CA where the overall mean slope is 7.8; the median percentage of burrowers is 8.0% - 15.0% in the other three bioregions where the overall mean slopes are steeper and the range of HUC12 watershed mean slopes is comparatively large. Medians of a metricâ€™s bioregion-specific Reference values are used as scoring thresholds in all eight index methods. Therefore, in methods where % Burrower is included in the index, % Burrower will score 100 when its values are < 8.0% in BLUE, < 15.0% in NCA and SRV, and < 24.0% in CA. Metric scoring in each bioregion is thus tailored to that bioregionâ€™s Reference and Degraded assemblages. So, although metric scores of 100 always represent Reference conditions in a given bioregion, they are not necessarily directly comparable across the twelve bioregions.

Despite the sometimes large differences in metric scoring thresholds between bioregions, jackknife results suggest the influence of these differences on the final index may be small. RMSE across all methods are only slightly higher in the jackknife CV results compared to the jackknife precision results (Figure I-2). This suggests variability associated with the index itself â€“ its component metrics and their thresholds â€“ is small. None of the methods appear to be overfitting to the data because differences between the original CE values and the jackknife CV mean CE are small. Method C had the largest difference (ğ‘¥Ì… = 4.4%); Method F had the smallest difference (ğ‘¥Ì… = 0.3%).

In Methods A â€“ E, some of the variability expressed in RMSE also may be due to random variability introduced by the rarefaction procedure used in data preparation (Appendix D). As mentioned in the report, the rarefaction program selects in descending order the most frequently occurring taxa in a sample and builds a standardized sample (100 organisms in this study). When faced with equally frequent taxa near the end of the selection process, the program randomly selects taxa until a count of 100 is reached. The effect of this random selection can be slight differences in metric values produced by multiple program runs, and hence slight differences in the index CEâ€™s.
Method A was designated the top performing method because it had the greatest CV mean CE value (ğ‘¥Ì… = 75.8%) and a relatively low mean difference between the CE value calculated using all the available data (ğ‘¥Ì… = 1.4%). Although Method A was not always the top performing index in all bioregions, the CV mean CE was consistently within 5.0% of the top performing methodsâ€™ CV mean CE value.

Method A was not as precise as Methods B and C, but all three methods produced a mean jackknife CE within 2.0% of one another (Table I-3). The weight of each metric in the final index score was substantially lower for Methods B and C compared to Method A. Method A is more susceptible to random variability of individual metrics. Methods B and C reduce variability by increasing the number of metrics contributing to the final index score; therefore, random variability of individual metrics is smoothed by the large number of contributing metrics. However, the increase in precision estimate provided by Methods B and C was not considered a significant improvement from the precision estimate of Method A.

Method A follows the general index development strategy outlined in literature (Barbour et al. 1996, Gerritsen et al. 2000b, Pond et al. 2011). Our assessment does not justify the use of any modified index development methodology. Additionally, following the principle of parsimony, Method A in general requires fewer metrics and is a more simplistic model than the other methods assessed (Hawkins 2004); therefore, Method A should be preferred to other methods with similar results. Finally, it would be inappropriate to apply different index development methods to each bioregion. Each bioregion index will be subjected to a standard categorical rating system for the Chesapeake Bay basin assessment. The results do not provide a clear pattern of method performance, suggesting unique biases among methodologies. Standardizing the results of multiple index development methodologies may provide skewed results. Therefore, Method A was used to develop all indices used in the Chessie BIBI assessment.
Range, variability, and redundancy assessments inform the R-programâ€™s metric selections in Methods A, D, and E. Metrics selected for an index should reflect ecological responses to environmental degradation and not â€œoverfitâ€ the data by responding to noise in the data (Barbour et al. 1999). Setting standards for metric range and variability can protect against over fitting the index. For range and variability assessments only the Reference data are used. Blocksom and Johnson (2009) calculated range as the difference between the minimum and maximum metric values. To avoid outlier influences, we calculated range as the difference between the Reference 5th and 95th percentiles. For richness metrics a range greater than 5 was required. Percent metrics required a range greater than or equal to 10.0%. Selecting metrics with low range restricts the Reference criteria beyond expected natural variability (i.e., in effect creating high probability for false-negatives). Measuring variability acts as a counter measure to range. Preferably metrics with high range and low variability are selected for further analysis. Variability was measured as Reference interquartile range (IQR) relative to the range between 0 and the Reference 25th percentile (Blocksom and Johnson 2009). Metrics were selected if the IQR was not greater than the range between 0 to the Reference 25th percentile.

Spearman correlation was used to assess and remove redundant metrics in Methods A, D, and E. Including two significantly correlated metrics (r â‰¥ 0.85 or â‰¤ -0.85) in an index would be like including a single metric twice. An r value of 0.85 is relatively high but has been used in other indices (Gerritsen et al. 2000a, Butcher et al. 2003) and indicates ~72.0% of metric values are in common among two metrics of interest. Redundant metrics (r â‰¥ 0.85 or â‰¤ -0.85) were compared pairwise using a Wilcoxon-Rank Sum test. No Î±-value was specified because the test was only used to indicate which metric had greater separation between the Reference and Degraded distributions. The metric with the lower p-value were retained. The metrics remaining after the redundancy analysis were considered for the index.

A common issue in ecological data is community compositions dominated by zeros. Underlying differences in the distributions of Reference and Degraded values may be masked by the dominance of zeros. If more than 5% of Reference sample values were zero, only the subset of metrics with values greater than zero were used to evaluate sensitivity. If the sample size in each metric category after the removal of all zero values was deemed acceptable and the metric was considered sensitive, the metric was held for further review within the index. This â€œzero inflation protectionâ€ was applied in Methods C, D, E, F, G, and H. Metrics selected in Method A did not experience a zero problem.




