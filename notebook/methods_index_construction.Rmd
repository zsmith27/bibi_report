---
title: "Index Construction"
output: html_notebook
---

## Index Construction

Eight ways of constructing a multi-metric index from family-level metrics were examined (Appendix I). The purpose of the exercise was to gain insights into the consequences of choosing a specific index structure or combination of metrics. In one extreme, only the single most sensitive metric from each of five metric categories (richness/diversity, tolerance, functional feeding group, habit, and composition) was incorporated into the index. In another extreme, over 200 metrics were included. Zero inflation protection was applied in some methods and not in others. Zeros can be abundant in biological data and large proportion of zeros has the potential to mask relationships between environmental and biological data (McCune et al. 2002). For three methods, several metrics in each of the five metric categories were pre-selected or selected as most sensitive by the R-program and their scores averaged; subsequently, the five category averages were averaged to obtain a final index score. All eight methods were tested in the twelve new bioregions. We concluded that the development strategy closest to strategies described in the literature was the most practical (Method A in Appendix I).

Order, family, and genus-level indices were developed for each of the three spatial scales: Chesapeake-wide, region, and bioregion. For the family- and genus-level indices, metrics remaining after the range, variability, and redundancy checks were divided into the five metric categories and the four most sensitive metrics of the five categories were selected to be the foundation of the index using the metric testing and scoring approaches described above. Therefore, one category was not represented initially. Additional sensitive metrics were then added to the index only if they improved the index CE. Metrics omitted during the selection of the top four metrics were readmitted if the metric improved the index CE.

Functional feeding group (FFG), habit, and some tolerance metrics are inappropriate at the order-level. Thus, only richness/diversity, composition, and a subset of tolerance metrics were considered for the order-level version of the indices. When applicable, one richness/diversity metric, one composition metric, and one tolerance metric were required for order-level indices. After conducting range, variability, and redundancy checks, it was possible to have a very small set of potential metrics that represented a single metric type, and therefore, it was not possible to represent the three metric types. Additional sensitive metrics were then included if they improved index CE.

Within the Chessie BIBI database there are sampling stations that were revisited multiple times during a year or revisited during subsequent years. The scores of multiple sampling events for a station were not averaged during index development. Approximately 2,154 stations (10.1%) in the analysis dataset have between 2 and 35 sampling events each. The index scores for the sampling events at one station have the potential to vary considerably, and narrative ratings (described below) at some stations occasionally ranged across all five rating categories, from Excellent to Very Poor. At some stations, variation was associated with changes in stream water quality and habitat conditions; in others, it appeared to be natural inter-annual variability. It was assumed that each sampling event represents the biological response to the immediate environment and not the previous year’s biological status.
However, treating these repeat-visit sampling events as independent samples could result in pseudoreplication. Sampling events collected from the same sampling stations at different times have a higher probability of not being statistically independent. For example, two sampling events collected from the same sampling station two years apart have a higher probability of representing a more similar macroinvertebrate community than two sampling events collected at different sampling stations. This pseudoreplication could create a bias towards the community found at repeatedly sampled stations. To minimize the potential influence of pseudoreplication, each sampling station was represented by a single sampling event. If multiple sampling events were recorded for a single sampling station, then one sampling event was selected at random. This process should have eliminated pseudoreplicates at a given station, but pseudoreplication may still be an issue in this dataset. In some instances, multiple agencies collected samples from the same sampling station but assigned different station names. Currently, sampling stations with two or more station names are difficult to find in the database. Additionally, some of the data represents intensively sampled streams, where the sampling stations are different but the distance between these stations is small. In these cases, the probability of these sampling events representing statistically independent samples is low. Although this situation appears to represent pseudoreplication it may be more appropriate to refer to this a spatial autocorrelation. It was assumed that the occurrence of these pseudoreplicates and spatially autocorrelated sampling events was infrequent but GIS assessment in future refinements of the Chessie BIBI may be able to identify and manage these potential issues. As the sample size increases, the influence of relatively small set of pseudoreplicates or spatially autocorrelated sampling events should diminish because each sampling event has less weight and the number of independent samples represents a majority of the data. We contend that the large sample size of the Chessie BIBI dataset and the process described above for managing sampling station pseudoreplicates minimizes any potential bias associated with pseudoreplication.

Metrics were scored using the Balanced Discrimination Efficiency (BDE) approach described in II. F. G. Metric Scoring Approach. This method is compared to three others in Appendix J. Each index produces different scores but their ability to correctly classify Reference and Degraded sites (classification efficiency) is essentially the same. The Balanced Discrimination Efficiency method was best at spreading index scores across the entire 0 – 100 continuous scoring scale.

Sampling events and sampling stations were not evenly distributed throughout the Chesapeake Bay basin. Maryland had broad coverage and a high density of sampling events, in part, due to their Stream Waders volunteer program. The high density of sampling events in certain areas, such as Maryland, may create indices biased towards the conditions of that area. To reduce the potential bias when creating the regional and basin-wide indices, fifty Reference and fifty Degraded sampling events were randomly selected from each bioregion. If the bioregion had less than fifty Reference or less than fifty Degraded sampling events, then all the sampling events where retained during the assessment. Reducing the Reference and Degraded sample sizes to a maximum of fifty reduced the potential for spatial bias, e.g., a basin-wide or regional index developed with one bioregion containing two or three times the number of Reference sampling events relative to the other bioregions. After the bioregions were subset to a maximum of fifty Reference and fifty Degraded sampling events, they were aggregated to the appropriate basin-wide or region spatial resolutions.

In repeated runs of the R-program scripts to select and test metrics for the bioregion and region indices, we noticed that random choices made early in the data preparation’s probabilistic rarefaction step affected the metric selections and scoring thresholds. The probabilistic rarefaction process reduces the variability inherent in the general rarefaction process; however, in each run it randomly selects from equally rare taxa to make up a sample count of about one-hundred. Slight differences in the results influence metric redundancy, range, variability, and sensitivity, and ultimately affect which metrics are selected (see Appendix D). The richness/diversity metrics appear to be the only metrics with the potential to change scoring thresholds with each run due to the probabilistic rarefaction process. Additionally, with each new run of the R-program scripts the random selection of a single sampling event to represent each station and/or the random sub-setting of Reference and Degraded, described in the previous paragraphs, added additional variability to the metrics selected for the final index and the associated scoring thresholds.

To reduce the potential error created by running the program only once, an iterative development process was adopted. R-program scripts were developed to automate the entire process associated with index development: the random selection of a single sampling event to represent each sampling station, if applicable a maximum of fifty Reference and fifty Degraded samples selected at random from each bioregion, metric calculations, metric range test, metric variability test, metric sensitivity test, metric redundancy test, metric selection (based on metric type, range, variability, sensitivity, and redundancy), metric scoring, and averaging the scores into the final index score. We then identified the metrics that occurred most often in fifty runs and incorporated them into the final indices. Metrics occurring in 20% or more of the fifty runs were included in the final indices. If fewer than five metrics occurred in 20% or more of the runs, the metrics were ranked in descending order and the frequency of the fifth ranked metric was used as the new frequency threshold to select metrics. The selected metrics were subjected to a final metric redundancy assessment using all the available data. Any metrics identified as redundant were removed following the procedure outlined in II. F. iii. Metric Redundancy. The means of the metric scoring thresholds calculated in the fifty runs were used as the metric scoring thresholds in the final indices. This iterative process provides more robust indices that are less susceptible to overfitting the indices and more sensitive to the underlying biological patterns associated with the defined disturbance gradient.

To calculate the index score of a sampling event, the metrics corresponding to the selected spatial (basin-wide, region, bioregion) and taxonomic (order, family, genus) index are scored and averaged.